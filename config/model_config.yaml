# config/model_config.yaml

data:
  train_size: 0.8
  random_state: 42
  max_features: 20000
  max_len: 512

preprocessing:
  min_df: 5
  stop_words: english
  token_pattern: '\b[a-zA-Z]{3,}\b'

bert:
  model_name: 'bert-base-uncased'
  learning_rate: 2e-5
  batch_size: 16
  num_epochs: 3
  max_length: 512

transformer_models:
  xlnet:
    model_name: 'xlnet-base-cased'
  distilbert:
    model_name: 'distilbert-base-uncased'
  roberta:
    model_name: 'roberta-base'
  albert:
    model_name: 'albert-base-v2'

traditional_models:
  random_forest:
    n_estimators: 100
    random_state: 42
  svm:
    kernel: 'linear'
    max_iter: 1000
  logistic_regression:
    max_iter: 1000