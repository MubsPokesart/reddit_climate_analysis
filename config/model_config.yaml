model_parameters:
  bert:
    pretrained_model: 'bert-base-uncased'
    max_length: 512
    batch_size: 16
    learning_rate: 2e-5
    num_epochs: 3
    
  traditional:
    random_forest:
      n_estimators: 100
      random_state: 42
    svm:
      max_iter: 1000
    logistic_regression:
      max_iter: 1000
      
  word_embeddings:
    vocab_size: 20000
    min_df: 5
    embedding_dim: 300